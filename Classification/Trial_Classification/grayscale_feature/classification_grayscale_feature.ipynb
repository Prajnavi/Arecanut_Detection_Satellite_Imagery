{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGtNeNDKdn3TX5XtMIVgpW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"j5GAojlC2rUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qneOrTms1qcE","executionInfo":{"status":"ok","timestamp":1744353797758,"user_tz":-330,"elapsed":29436,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"6cae9df6-e439-4b31-bacd-d1dfe7ae3dce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["OUTPUT_FOLDER = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/grayscale_feature/labelled_images\"\n","os.makedirs(OUTPUT_FOLDER, exist_ok=True)"],"metadata":{"id":"hfKPIlEH2CrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading stored reference features from CSV\n","def load_reference_features(csv_file):\n","    return pd.read_csv(csv_file, header=None).values.flatten()"],"metadata":{"id":"HWHEB7U525xZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting features from an image\n","def extract_features(image_path):\n","    img = cv2.imread(image_path)\n","    if img is None:\n","        print(f\"Error: Could not load image {image_path}\")\n","        return None\n","\n","    img = cv2.resize(img, (128, 128))  # Resize for consistency\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # GLCM Texture Features\n","    from skimage.feature import graycomatrix, graycoprops # Import graycomatrix and graycoprops\n","    glcm = graycomatrix(gray, distances=[5], angles=[0], symmetric=True, normed=True)\n","    contrast = graycoprops(glcm, 'contrast')[0, 0]\n","    correlation = graycoprops(glcm, 'correlation')[0, 0]\n","    energy = graycoprops(glcm, 'energy')[0, 0]\n","    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n","\n","    # Compute histogram (color features)\n","    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n","\n","    # Edge detection (Canny)\n","    edges = cv2.Canny(gray, 100, 200)\n","    edge_density = np.sum(edges) / (128 * 128)\n","\n","    # Combine features (texture + color + edge) to match the original feature extraction\n","    return np.hstack([contrast, correlation, energy, homogeneity, hist, edge_density])"],"metadata":{"id":"IA3lbrb73T82"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Classifing and rename images\n","def classify_and_rename_images(image_folder, feature_csv, output_folder, threshold=0.8):\n","    # Loading stored features from CSV\n","    reference_features = load_reference_features(feature_csv)\n","\n","    # Checking output folder exists\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    # Iterating over all images in the folder\n","    for filename in os.listdir(image_folder):\n","        image_path = os.path.join(image_folder, filename)\n","\n","        # Extracting features for the current image\n","        features = extract_features(image_path)\n","        if features is None:\n","            continue  # Skiping if feature extraction failed\n","\n","        # Computing similarity (Cosine Similarity)\n","        similarity = np.dot(reference_features, features) / (np.linalg.norm(reference_features) * np.linalg.norm(features))\n","\n","        # Assigning labels\n","        label = \"1\" if similarity >= threshold else \"0\"\n","\n","        # Renaming and saving the image\n","        new_filename = f\"{label}_{filename}\"\n","        new_path = os.path.join(output_folder, new_filename)\n","        cv2.imwrite(new_path, cv2.imread(image_path))\n","\n","        print(f\"Processed: {filename} → {new_filename} (Similarity: {similarity:.2f})\")"],"metadata":{"id":"ykMZfAIs3a2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/splitted_images\"\n","feature_file = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/grayscale_feature/extracted_grayscale_feature.csv\"\n","output_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/grayscale_feature/labelled_images\"\n","\n","# Classify images\n","classify_and_rename_images(image_folder, feature_file, output_folder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ifc1ycqN3fUc","executionInfo":{"status":"ok","timestamp":1744353866690,"user_tz":-330,"elapsed":7140,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"e346f913-de0b-4647-8348-764cf6eeef0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed: image_1_tile_0.png → 1_image_1_tile_0.png (Similarity: 0.95)\n","Processed: image_1_tile_1.png → 1_image_1_tile_1.png (Similarity: 0.95)\n","Processed: image_1_tile_2.png → 1_image_1_tile_2.png (Similarity: 0.87)\n","Processed: image_1_tile_3.png → 1_image_1_tile_3.png (Similarity: 0.82)\n","Processed: image_1_tile_4.png → 1_image_1_tile_4.png (Similarity: 0.91)\n","Processed: image_1_tile_5.png → 1_image_1_tile_5.png (Similarity: 0.88)\n","Processed: image_1_tile_6.png → 0_image_1_tile_6.png (Similarity: 0.70)\n","Processed: image_1_tile_7.png → 0_image_1_tile_7.png (Similarity: 0.80)\n","Processed: image_1_tile_8.png → 1_image_1_tile_8.png (Similarity: 0.93)\n","Processed: image_1_tile_9.png → 1_image_1_tile_9.png (Similarity: 0.96)\n","Processed: image_1_tile_10.png → 1_image_1_tile_10.png (Similarity: 0.96)\n","Processed: image_1_tile_11.png → 1_image_1_tile_11.png (Similarity: 0.86)\n","Processed: image_1_tile_12.png → 1_image_1_tile_12.png (Similarity: 0.84)\n","Processed: image_1_tile_13.png → 0_image_1_tile_13.png (Similarity: 0.63)\n","Processed: image_1_tile_14.png → 0_image_1_tile_14.png (Similarity: 0.77)\n","Processed: image_1_tile_15.png → 1_image_1_tile_15.png (Similarity: 0.92)\n","Processed: image_1_tile_16.png → 1_image_1_tile_16.png (Similarity: 0.95)\n","Processed: image_1_tile_17.png → 1_image_1_tile_17.png (Similarity: 0.92)\n","Processed: image_1_tile_18.png → 0_image_1_tile_18.png (Similarity: 0.74)\n","Processed: image_1_tile_19.png → 0_image_1_tile_19.png (Similarity: 0.67)\n","Processed: image_1_tile_20.png → 0_image_1_tile_20.png (Similarity: 0.72)\n","Processed: image_1_tile_21.png → 0_image_1_tile_21.png (Similarity: 0.77)\n","Processed: image_1_tile_22.png → 1_image_1_tile_22.png (Similarity: 0.81)\n","Processed: image_1_tile_23.png → 0_image_1_tile_23.png (Similarity: 0.72)\n","Processed: image_1_tile_24.png → 1_image_1_tile_24.png (Similarity: 0.88)\n","Processed: image_1_tile_25.png → 1_image_1_tile_25.png (Similarity: 0.93)\n","Processed: image_1_tile_26.png → 1_image_1_tile_26.png (Similarity: 0.80)\n","Processed: image_1_tile_27.png → 0_image_1_tile_27.png (Similarity: 0.80)\n","Processed: image_4_tile_0.png → 1_image_4_tile_0.png (Similarity: 0.98)\n","Processed: image_4_tile_1.png → 1_image_4_tile_1.png (Similarity: 0.99)\n","Processed: image_4_tile_2.png → 1_image_4_tile_2.png (Similarity: 0.99)\n","Processed: image_4_tile_3.png → 1_image_4_tile_3.png (Similarity: 0.93)\n","Processed: image_4_tile_4.png → 0_image_4_tile_4.png (Similarity: 0.79)\n","Processed: image_4_tile_5.png → 1_image_4_tile_5.png (Similarity: 0.84)\n","Processed: image_4_tile_6.png → 0_image_4_tile_6.png (Similarity: 0.41)\n","Processed: image_4_tile_7.png → 1_image_4_tile_7.png (Similarity: 0.89)\n","Processed: image_4_tile_8.png → 1_image_4_tile_8.png (Similarity: 1.00)\n","Processed: image_4_tile_9.png → 1_image_4_tile_9.png (Similarity: 0.99)\n","Processed: image_4_tile_10.png → 1_image_4_tile_10.png (Similarity: 0.99)\n","Processed: image_4_tile_11.png → 1_image_4_tile_11.png (Similarity: 0.98)\n","Processed: image_4_tile_12.png → 0_image_4_tile_12.png (Similarity: 0.71)\n","Processed: image_4_tile_13.png → 0_image_4_tile_13.png (Similarity: 0.80)\n","Processed: image_4_tile_14.png → 0_image_4_tile_14.png (Similarity: 0.42)\n","Processed: image_4_tile_15.png → 1_image_4_tile_15.png (Similarity: 0.95)\n","Processed: image_4_tile_16.png → 1_image_4_tile_16.png (Similarity: 0.99)\n","Processed: image_4_tile_17.png → 1_image_4_tile_17.png (Similarity: 1.00)\n","Processed: image_4_tile_18.png → 1_image_4_tile_18.png (Similarity: 1.00)\n","Processed: image_4_tile_19.png → 1_image_4_tile_19.png (Similarity: 0.99)\n","Processed: image_4_tile_20.png → 1_image_4_tile_20.png (Similarity: 0.93)\n","Processed: image_4_tile_21.png → 0_image_4_tile_21.png (Similarity: 0.07)\n","Processed: image_4_tile_22.png → 0_image_4_tile_22.png (Similarity: 0.38)\n","Processed: image_4_tile_23.png → 1_image_4_tile_23.png (Similarity: 0.85)\n","Processed: image_4_tile_24.png → 1_image_4_tile_24.png (Similarity: 0.99)\n","Processed: image_4_tile_25.png → 1_image_4_tile_25.png (Similarity: 0.98)\n","Processed: image_4_tile_26.png → 1_image_4_tile_26.png (Similarity: 0.99)\n","Processed: image_4_tile_27.png → 0_image_4_tile_27.png (Similarity: 0.68)\n","Processed: image_2_tile_0.png → 0_image_2_tile_0.png (Similarity: 0.66)\n","Processed: image_2_tile_1.png → 1_image_2_tile_1.png (Similarity: 0.90)\n","Processed: image_2_tile_2.png → 0_image_2_tile_2.png (Similarity: 0.76)\n","Processed: image_2_tile_3.png → 0_image_2_tile_3.png (Similarity: 0.74)\n","Processed: image_2_tile_4.png → 0_image_2_tile_4.png (Similarity: 0.78)\n","Processed: image_2_tile_5.png → 0_image_2_tile_5.png (Similarity: 0.61)\n","Processed: image_2_tile_6.png → 0_image_2_tile_6.png (Similarity: 0.36)\n","Processed: image_2_tile_7.png → 0_image_2_tile_7.png (Similarity: 0.74)\n","Processed: image_2_tile_8.png → 1_image_2_tile_8.png (Similarity: 0.88)\n","Processed: image_2_tile_9.png → 0_image_2_tile_9.png (Similarity: 0.77)\n","Processed: image_2_tile_10.png → 0_image_2_tile_10.png (Similarity: 0.69)\n","Processed: image_2_tile_11.png → 0_image_2_tile_11.png (Similarity: 0.63)\n","Processed: image_2_tile_12.png → 0_image_2_tile_12.png (Similarity: 0.06)\n","Processed: image_2_tile_13.png → 1_image_2_tile_13.png (Similarity: 0.82)\n","Processed: image_2_tile_14.png → 0_image_2_tile_14.png (Similarity: 0.51)\n","Processed: image_2_tile_15.png → 0_image_2_tile_15.png (Similarity: 0.49)\n","Processed: image_2_tile_16.png → 1_image_2_tile_16.png (Similarity: 0.83)\n","Processed: image_2_tile_17.png → 0_image_2_tile_17.png (Similarity: 0.75)\n","Processed: image_2_tile_18.png → 0_image_2_tile_18.png (Similarity: 0.53)\n","Processed: image_2_tile_19.png → 0_image_2_tile_19.png (Similarity: 0.14)\n","Processed: image_2_tile_20.png → 0_image_2_tile_20.png (Similarity: 0.78)\n","Processed: image_2_tile_21.png → 0_image_2_tile_21.png (Similarity: 0.48)\n","Processed: image_2_tile_22.png → 0_image_2_tile_22.png (Similarity: 0.45)\n","Processed: image_2_tile_23.png → 1_image_2_tile_23.png (Similarity: 0.81)\n","Processed: image_2_tile_24.png → 1_image_2_tile_24.png (Similarity: 0.89)\n","Processed: image_2_tile_25.png → 0_image_2_tile_25.png (Similarity: 0.39)\n","Processed: image_2_tile_26.png → 0_image_2_tile_26.png (Similarity: 0.68)\n","Processed: image_2_tile_27.png → 1_image_2_tile_27.png (Similarity: 0.82)\n","Processed: image_3_tile_0.png → 0_image_3_tile_0.png (Similarity: 0.50)\n","Processed: image_3_tile_1.png → 1_image_3_tile_1.png (Similarity: 0.83)\n","Processed: image_3_tile_2.png → 1_image_3_tile_2.png (Similarity: 0.85)\n","Processed: image_3_tile_3.png → 0_image_3_tile_3.png (Similarity: 0.34)\n","Processed: image_3_tile_4.png → 0_image_3_tile_4.png (Similarity: 0.20)\n","Processed: image_3_tile_5.png → 0_image_3_tile_5.png (Similarity: 0.60)\n","Processed: image_3_tile_6.png → 0_image_3_tile_6.png (Similarity: 0.50)\n","Processed: image_3_tile_7.png → 0_image_3_tile_7.png (Similarity: 0.28)\n","Processed: image_3_tile_8.png → 1_image_3_tile_8.png (Similarity: 0.86)\n","Processed: image_3_tile_9.png → 1_image_3_tile_9.png (Similarity: 0.86)\n","Processed: image_3_tile_10.png → 0_image_3_tile_10.png (Similarity: 0.57)\n","Processed: image_3_tile_11.png → 0_image_3_tile_11.png (Similarity: 0.47)\n","Processed: image_3_tile_12.png → 0_image_3_tile_12.png (Similarity: 0.70)\n","Processed: image_3_tile_13.png → 0_image_3_tile_13.png (Similarity: 0.15)\n","Processed: image_3_tile_14.png → 0_image_3_tile_14.png (Similarity: 0.66)\n","Processed: image_3_tile_15.png → 0_image_3_tile_15.png (Similarity: 0.63)\n","Processed: image_3_tile_16.png → 1_image_3_tile_16.png (Similarity: 0.85)\n","Processed: image_3_tile_17.png → 1_image_3_tile_17.png (Similarity: 0.88)\n","Processed: image_3_tile_18.png → 0_image_3_tile_18.png (Similarity: 0.72)\n","Processed: image_3_tile_19.png → 1_image_3_tile_19.png (Similarity: 0.91)\n","Processed: image_3_tile_20.png → 0_image_3_tile_20.png (Similarity: 0.43)\n","Processed: image_3_tile_21.png → 0_image_3_tile_21.png (Similarity: 0.08)\n","Processed: image_3_tile_22.png → 0_image_3_tile_22.png (Similarity: 0.37)\n","Processed: image_3_tile_23.png → 0_image_3_tile_23.png (Similarity: 0.60)\n","Processed: image_3_tile_24.png → 1_image_3_tile_24.png (Similarity: 0.95)\n","Processed: image_3_tile_25.png → 1_image_3_tile_25.png (Similarity: 0.84)\n","Processed: image_3_tile_26.png → 0_image_3_tile_26.png (Similarity: 0.65)\n","Processed: image_3_tile_27.png → 0_image_3_tile_27.png (Similarity: 0.57)\n"]}]},{"cell_type":"code","source":["!pip install scikit-image opencv-python pandas\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from skimage.feature import graycomatrix, graycoprops\n","from sklearn.metrics import accuracy_score, classification_report\n","from google.colab import drive # Make sure this is imported"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBqfB5lGl8vT","executionInfo":{"status":"ok","timestamp":1744354007020,"user_tz":-330,"elapsed":5390,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"b8da126e-8960-4861-ca15-b9ccb92d0b4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["# Function to check accuracy against manually labeled images\n","def check_accuracy(manual_folder, predicted_folder):\n","    # Lists to store true and predicted labels\n","    y_true = []\n","    y_pred = []\n","\n","    # Get manually labeled images\n","    manual_files = {f for f in os.listdir(manual_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))}\n","\n","    # Get predicted labeled images\n","    predicted_files = {f for f in os.listdir(predicted_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))}\n","\n","    # Compare labels\n","    for manual_file in manual_files:\n","        # Extract true label from filename (assuming format: \"1_image.png\" or \"0_image.png\")\n","        true_label = manual_file.split('_')[0]\n","        original_name = '_'.join(manual_file.split('_')[1:])  # Get original filename without label\n","\n","        # Find corresponding predicted file\n","        predicted_name = f\"1_{original_name}\" if f\"1_{original_name}\" in predicted_files else f\"0_{original_name}\"\n","        if predicted_name in predicted_files:\n","            pred_label = predicted_name.split('_')[0]\n","            y_true.append(int(true_label))\n","            y_pred.append(int(pred_label))\n","        else:\n","            print(f\"Warning: No prediction found for {manual_file}\")\n","\n","    # Compute accuracy\n","    if y_true and y_pred:\n","        accuracy = accuracy_score(y_true, y_pred)\n","        report = classification_report(y_true, y_pred, target_names=['Negative (0)', 'Positive (1)'])\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(\"Classification Report:\\n\", report)\n","    else:\n","        print(\"Error: No matching labels found for comparison!\")"],"metadata":{"id":"wtCoajtAmfQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/grayscale_feature/labelled_images\"\n","manual_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/manually_labelled_images\"\n","\n","print(\"Manual folder path:\", manual_folder)\n","\n","# Check if the directory exists\n","if not os.path.exists(manual_folder):\n","  print(f\"Error: Manual folder not found: {manual_folder}\")\n","\n","#Check accuracy against manually labeled images\n","check_accuracy(manual_folder, output_folder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_JDs39mmjok","executionInfo":{"status":"ok","timestamp":1744354062091,"user_tz":-330,"elapsed":333,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"b07b5f3c-4ae7-44c9-95da-24fd3b29af05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual folder path: /content/gdrive/MyDrive/AI_WRM/Classification/Trial1_Classification/manually_labelled_images\n","Accuracy: 0.7768\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","Negative (0)       0.60      0.95      0.74        37\n","Positive (1)       0.96      0.69      0.81        75\n","\n","    accuracy                           0.78       112\n","   macro avg       0.78      0.82      0.77       112\n","weighted avg       0.84      0.78      0.78       112\n","\n"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIEfDmMVXPRh0BbF6g/U1F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0eohsMNxcfPV"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from skimage.feature import graycomatrix, graycoprops\n","from google.colab import drive"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsAsmWf4cizO","executionInfo":{"status":"ok","timestamp":1744279369575,"user_tz":-330,"elapsed":4499,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"274e7c96-9e57-4e27-b239-ad8a008d588b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["OUTPUT_FOLDER = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/RGB_feature/labelled_images\"\n","os.makedirs(OUTPUT_FOLDER, exist_ok=True)"],"metadata":{"id":"UysBl5ECctBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading stored reference features from CSV\n","def load_reference_features(csv_file):\n","    return pd.read_csv(csv_file, header=None).values.flatten()"],"metadata":{"id":"LP8jywPac6h5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to extract texture and color features\n","def extract_features(image_path, distance=5):\n","    img = cv2.imread(image_path)\n","\n","    # Checking if the image is there\n","    if img is None:\n","        print(f\"Error: Could not load image from {image_path}\")\n","        return None\n","    img = cv2.resize(img, (128, 128))  # Resize for consistency\n","\n","    # Use green channel (index 1 in BGR) instead of converting to grayscale\n","    green_channel = img[:, :, 1]  # BGR format in OpenCV, 1 is green\n","\n","    # GLCM Texture Features using green channel\n","    glcm = graycomatrix(green_channel, distances=[distance], angles=[0], symmetric=True, normed=True)\n","    contrast = graycoprops(glcm, 'contrast')[0, 0]\n","    correlation = graycoprops(glcm, 'correlation')[0, 0]\n","    energy = graycoprops(glcm, 'energy')[0, 0]\n","    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n","\n","    # Color Histogram Features (RGB)\n","    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n","\n","    # Edge Detection (Canny) using green channel\n","    edges = cv2.Canny(green_channel, 100, 200)\n","    edge_density = np.sum(edges) / (128 * 128)  # Ratio of edge pixels\n","\n","    return np.hstack([contrast, correlation, energy, homogeneity, hist, edge_density])\n","\n","# Test with different distance values\n","def test_distances(image_path):\n","    distances = [1, 3, 5, 7, 10]  # Different distance values to test\n","    results = {}\n","\n","    for dist in distances:\n","        features = extract_features(image_path, distance=dist)\n","        if features is not None:\n","            results[dist] = {\n","                'contrast': features[0],\n","                'correlation': features[1],\n","                'energy': features[2],\n","                'homogeneity': features[3],\n","                'edge_density': features[-1]\n","            }\n","            print(f\"Distance {dist}:\")\n","            print(f\"Contrast: {features[0]:.4f}\")\n","            print(f\"Correlation: {features[1]:.4f}\")\n","            print(f\"Energy: {features[2]:.4f}\")\n","            print(f\"Homogeneity: {features[3]:.4f}\")\n","            print(f\"Edge Density: {features[-1]:.4f}\")\n","            print(\"--------------------\")\n","\n","    return results"],"metadata":{"id":"MS_gMQYwc-rk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_reference_features(feature_csv):\n","    try:\n","        df = pd.read_csv(feature_csv, header=None)\n","        print(\"Loaded CSV shape:\", df.shape)\n","        if df.empty:\n","            raise ValueError(\"CSV file is empty!\")\n","        features = df.values.flatten()\n","        print(\"Loaded features length:\", len(features))\n","        if len(features) != 517:\n","            raise ValueError(f\"Expected 517 features, got {len(features)}\")\n","        return features\n","    except Exception as e:\n","        print(f\"Error loading reference features: {e}\")\n","        return None\n","\n","def classify_and_rename_images(image_folder, feature_csv, output_folder, threshold=0.8, distance=5):\n","    reference_features = load_reference_features(feature_csv)\n","    if reference_features is None:\n","        print(\"Aborting classification due to reference feature loading failure!\")\n","        return\n","\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    for filename in os.listdir(image_folder):\n","        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n","            image_path = os.path.join(image_folder, filename)\n","            features = extract_features(image_path, distance=distance)\n","            if features is None:\n","                print(f\"Skipping {filename} - feature extraction failed\")\n","                continue\n","\n","            try:\n","                similarity = np.dot(reference_features, features) / (\n","                    np.linalg.norm(reference_features) * np.linalg.norm(features)\n","                )\n","                label = \"1\" if similarity >= threshold else \"0\"\n","                new_filename = f\"{label}_{filename}\"\n","                new_path = os.path.join(output_folder, new_filename)\n","                img = cv2.imread(image_path)\n","                if img is not None:\n","                    cv2.imwrite(new_path, img)\n","                    print(f\"Processed: {filename} → {new_filename} (Similarity: {similarity:.2f})\")\n","                else:\n","                    print(f\"Failed to read image: {filename}\")\n","            except ValueError as e:\n","                print(f\"Error computing similarity for {filename}: {e}\")\n","\n","# Paths\n","image_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/splitted_images\"\n","feature_file = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/RGB_feature/extracted_RGB_feature.csv\"\n","output_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/RGB_feature/labelled_images\"\n","\n","# Classify images\n","classify_and_rename_images(image_folder, feature_file, output_folder, threshold=0.8, distance=5)\n","\n","# Sync Drive\n","drive.flush_and_unmount()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbwiKyxxiyTl","executionInfo":{"status":"ok","timestamp":1744279403899,"user_tz":-330,"elapsed":23937,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"3e3c20a2-4a3b-4106-b3a3-3f4af9533ef3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded CSV shape: (1, 517)\n","Loaded features length: 517\n","Processed: image_1_tile_0.png → 1_image_1_tile_0.png (Similarity: 0.95)\n","Processed: image_1_tile_1.png → 1_image_1_tile_1.png (Similarity: 0.95)\n","Processed: image_1_tile_2.png → 1_image_1_tile_2.png (Similarity: 0.87)\n","Processed: image_1_tile_3.png → 1_image_1_tile_3.png (Similarity: 0.83)\n","Processed: image_1_tile_4.png → 1_image_1_tile_4.png (Similarity: 0.91)\n","Processed: image_1_tile_5.png → 1_image_1_tile_5.png (Similarity: 0.88)\n","Processed: image_1_tile_6.png → 0_image_1_tile_6.png (Similarity: 0.70)\n","Processed: image_1_tile_7.png → 1_image_1_tile_7.png (Similarity: 0.80)\n","Processed: image_1_tile_8.png → 1_image_1_tile_8.png (Similarity: 0.93)\n","Processed: image_1_tile_9.png → 1_image_1_tile_9.png (Similarity: 0.96)\n","Processed: image_1_tile_10.png → 1_image_1_tile_10.png (Similarity: 0.95)\n","Processed: image_1_tile_11.png → 1_image_1_tile_11.png (Similarity: 0.86)\n","Processed: image_1_tile_12.png → 1_image_1_tile_12.png (Similarity: 0.84)\n","Processed: image_1_tile_13.png → 0_image_1_tile_13.png (Similarity: 0.63)\n","Processed: image_1_tile_14.png → 0_image_1_tile_14.png (Similarity: 0.77)\n","Processed: image_1_tile_15.png → 1_image_1_tile_15.png (Similarity: 0.92)\n","Processed: image_1_tile_16.png → 1_image_1_tile_16.png (Similarity: 0.95)\n","Processed: image_1_tile_17.png → 1_image_1_tile_17.png (Similarity: 0.92)\n","Processed: image_1_tile_18.png → 0_image_1_tile_18.png (Similarity: 0.74)\n","Processed: image_1_tile_19.png → 0_image_1_tile_19.png (Similarity: 0.68)\n","Processed: image_1_tile_20.png → 0_image_1_tile_20.png (Similarity: 0.72)\n","Processed: image_1_tile_21.png → 0_image_1_tile_21.png (Similarity: 0.77)\n","Processed: image_1_tile_22.png → 1_image_1_tile_22.png (Similarity: 0.82)\n","Processed: image_1_tile_23.png → 0_image_1_tile_23.png (Similarity: 0.73)\n","Processed: image_1_tile_24.png → 1_image_1_tile_24.png (Similarity: 0.89)\n","Processed: image_1_tile_25.png → 1_image_1_tile_25.png (Similarity: 0.93)\n","Processed: image_1_tile_26.png → 1_image_1_tile_26.png (Similarity: 0.80)\n","Processed: image_1_tile_27.png → 0_image_1_tile_27.png (Similarity: 0.80)\n","Processed: image_4_tile_0.png → 1_image_4_tile_0.png (Similarity: 0.98)\n","Processed: image_4_tile_1.png → 1_image_4_tile_1.png (Similarity: 0.99)\n","Processed: image_4_tile_2.png → 1_image_4_tile_2.png (Similarity: 0.99)\n","Processed: image_4_tile_3.png → 1_image_4_tile_3.png (Similarity: 0.93)\n","Processed: image_4_tile_4.png → 0_image_4_tile_4.png (Similarity: 0.80)\n","Processed: image_4_tile_5.png → 1_image_4_tile_5.png (Similarity: 0.84)\n","Processed: image_4_tile_6.png → 0_image_4_tile_6.png (Similarity: 0.41)\n","Processed: image_4_tile_7.png → 1_image_4_tile_7.png (Similarity: 0.89)\n","Processed: image_4_tile_8.png → 1_image_4_tile_8.png (Similarity: 1.00)\n","Processed: image_4_tile_9.png → 1_image_4_tile_9.png (Similarity: 0.99)\n","Processed: image_4_tile_10.png → 1_image_4_tile_10.png (Similarity: 0.99)\n","Processed: image_4_tile_11.png → 1_image_4_tile_11.png (Similarity: 0.98)\n","Processed: image_4_tile_12.png → 0_image_4_tile_12.png (Similarity: 0.72)\n","Processed: image_4_tile_13.png → 1_image_4_tile_13.png (Similarity: 0.80)\n","Processed: image_4_tile_14.png → 0_image_4_tile_14.png (Similarity: 0.43)\n","Processed: image_4_tile_15.png → 1_image_4_tile_15.png (Similarity: 0.95)\n","Processed: image_4_tile_16.png → 1_image_4_tile_16.png (Similarity: 0.99)\n","Processed: image_4_tile_17.png → 1_image_4_tile_17.png (Similarity: 1.00)\n","Processed: image_4_tile_18.png → 1_image_4_tile_18.png (Similarity: 1.00)\n","Processed: image_4_tile_19.png → 1_image_4_tile_19.png (Similarity: 0.99)\n","Processed: image_4_tile_20.png → 1_image_4_tile_20.png (Similarity: 0.93)\n","Processed: image_4_tile_21.png → 0_image_4_tile_21.png (Similarity: 0.08)\n","Processed: image_4_tile_22.png → 0_image_4_tile_22.png (Similarity: 0.38)\n","Processed: image_4_tile_23.png → 1_image_4_tile_23.png (Similarity: 0.85)\n","Processed: image_4_tile_24.png → 1_image_4_tile_24.png (Similarity: 0.99)\n","Processed: image_4_tile_25.png → 1_image_4_tile_25.png (Similarity: 0.98)\n","Processed: image_4_tile_26.png → 1_image_4_tile_26.png (Similarity: 0.99)\n","Processed: image_4_tile_27.png → 0_image_4_tile_27.png (Similarity: 0.70)\n","Processed: image_2_tile_0.png → 0_image_2_tile_0.png (Similarity: 0.66)\n","Processed: image_2_tile_1.png → 1_image_2_tile_1.png (Similarity: 0.90)\n","Processed: image_2_tile_2.png → 0_image_2_tile_2.png (Similarity: 0.76)\n","Processed: image_2_tile_3.png → 0_image_2_tile_3.png (Similarity: 0.74)\n","Processed: image_2_tile_4.png → 0_image_2_tile_4.png (Similarity: 0.77)\n","Processed: image_2_tile_5.png → 0_image_2_tile_5.png (Similarity: 0.61)\n","Processed: image_2_tile_6.png → 0_image_2_tile_6.png (Similarity: 0.36)\n","Processed: image_2_tile_7.png → 0_image_2_tile_7.png (Similarity: 0.74)\n","Processed: image_2_tile_8.png → 1_image_2_tile_8.png (Similarity: 0.87)\n","Processed: image_2_tile_9.png → 0_image_2_tile_9.png (Similarity: 0.77)\n","Processed: image_2_tile_10.png → 0_image_2_tile_10.png (Similarity: 0.70)\n","Processed: image_2_tile_11.png → 0_image_2_tile_11.png (Similarity: 0.64)\n","Processed: image_2_tile_12.png → 0_image_2_tile_12.png (Similarity: 0.07)\n","Processed: image_2_tile_13.png → 1_image_2_tile_13.png (Similarity: 0.82)\n","Processed: image_2_tile_14.png → 0_image_2_tile_14.png (Similarity: 0.50)\n","Processed: image_2_tile_15.png → 0_image_2_tile_15.png (Similarity: 0.49)\n","Processed: image_2_tile_16.png → 1_image_2_tile_16.png (Similarity: 0.83)\n","Processed: image_2_tile_17.png → 0_image_2_tile_17.png (Similarity: 0.76)\n","Processed: image_2_tile_18.png → 0_image_2_tile_18.png (Similarity: 0.54)\n","Processed: image_2_tile_19.png → 0_image_2_tile_19.png (Similarity: 0.14)\n","Processed: image_2_tile_20.png → 0_image_2_tile_20.png (Similarity: 0.79)\n","Processed: image_2_tile_21.png → 0_image_2_tile_21.png (Similarity: 0.49)\n","Processed: image_2_tile_22.png → 0_image_2_tile_22.png (Similarity: 0.45)\n","Processed: image_2_tile_23.png → 1_image_2_tile_23.png (Similarity: 0.81)\n","Processed: image_2_tile_24.png → 1_image_2_tile_24.png (Similarity: 0.90)\n","Processed: image_2_tile_25.png → 0_image_2_tile_25.png (Similarity: 0.39)\n","Processed: image_2_tile_26.png → 0_image_2_tile_26.png (Similarity: 0.69)\n","Processed: image_2_tile_27.png → 1_image_2_tile_27.png (Similarity: 0.82)\n","Processed: image_3_tile_0.png → 0_image_3_tile_0.png (Similarity: 0.50)\n","Processed: image_3_tile_1.png → 1_image_3_tile_1.png (Similarity: 0.83)\n","Processed: image_3_tile_2.png → 1_image_3_tile_2.png (Similarity: 0.86)\n","Processed: image_3_tile_3.png → 0_image_3_tile_3.png (Similarity: 0.34)\n","Processed: image_3_tile_4.png → 0_image_3_tile_4.png (Similarity: 0.20)\n","Processed: image_3_tile_5.png → 0_image_3_tile_5.png (Similarity: 0.60)\n","Processed: image_3_tile_6.png → 0_image_3_tile_6.png (Similarity: 0.50)\n","Processed: image_3_tile_7.png → 0_image_3_tile_7.png (Similarity: 0.29)\n","Processed: image_3_tile_8.png → 1_image_3_tile_8.png (Similarity: 0.87)\n","Processed: image_3_tile_9.png → 1_image_3_tile_9.png (Similarity: 0.86)\n","Processed: image_3_tile_10.png → 0_image_3_tile_10.png (Similarity: 0.58)\n","Processed: image_3_tile_11.png → 0_image_3_tile_11.png (Similarity: 0.47)\n","Processed: image_3_tile_12.png → 0_image_3_tile_12.png (Similarity: 0.70)\n","Processed: image_3_tile_13.png → 0_image_3_tile_13.png (Similarity: 0.16)\n","Processed: image_3_tile_14.png → 0_image_3_tile_14.png (Similarity: 0.67)\n","Processed: image_3_tile_15.png → 0_image_3_tile_15.png (Similarity: 0.64)\n","Processed: image_3_tile_16.png → 1_image_3_tile_16.png (Similarity: 0.86)\n","Processed: image_3_tile_17.png → 1_image_3_tile_17.png (Similarity: 0.88)\n","Processed: image_3_tile_18.png → 0_image_3_tile_18.png (Similarity: 0.72)\n","Processed: image_3_tile_19.png → 1_image_3_tile_19.png (Similarity: 0.91)\n","Processed: image_3_tile_20.png → 0_image_3_tile_20.png (Similarity: 0.44)\n","Processed: image_3_tile_21.png → 0_image_3_tile_21.png (Similarity: 0.09)\n","Processed: image_3_tile_22.png → 0_image_3_tile_22.png (Similarity: 0.37)\n","Processed: image_3_tile_23.png → 0_image_3_tile_23.png (Similarity: 0.61)\n","Processed: image_3_tile_24.png → 1_image_3_tile_24.png (Similarity: 0.95)\n","Processed: image_3_tile_25.png → 1_image_3_tile_25.png (Similarity: 0.85)\n","Processed: image_3_tile_26.png → 0_image_3_tile_26.png (Similarity: 0.65)\n","Processed: image_3_tile_27.png → 0_image_3_tile_27.png (Similarity: 0.58)\n"]}]},{"cell_type":"code","source":["!pip install scikit-image opencv-python pandas\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from skimage.feature import graycomatrix, graycoprops\n","from sklearn.metrics import accuracy_score, classification_report\n","from google.colab import drive # Make sure this is imported"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QenfUy4-LOeM","executionInfo":{"status":"ok","timestamp":1744279765989,"user_tz":-330,"elapsed":2229,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"18c4f8f1-b4e2-43a0-f6f7-ab7382ff3d47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["# Function to check accuracy against manually labeled images\n","def check_accuracy(manual_folder, predicted_folder):\n","    # Lists to store true and predicted labels\n","    y_true = []\n","    y_pred = []\n","\n","    # Get manually labeled images\n","    manual_files = {f for f in os.listdir(manual_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))}\n","\n","    # Get predicted labeled images\n","    predicted_files = {f for f in os.listdir(predicted_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))}\n","\n","    # Compare labels\n","    for manual_file in manual_files:\n","        # Extract true label from filename (assuming format: \"1_image.png\" or \"0_image.png\")\n","        true_label = manual_file.split('_')[0]\n","        original_name = '_'.join(manual_file.split('_')[1:])  # Get original filename without label\n","\n","        # Find corresponding predicted file\n","        predicted_name = f\"1_{original_name}\" if f\"1_{original_name}\" in predicted_files else f\"0_{original_name}\"\n","        if predicted_name in predicted_files:\n","            pred_label = predicted_name.split('_')[0]\n","            y_true.append(int(true_label))\n","            y_pred.append(int(pred_label))\n","        else:\n","            print(f\"Warning: No prediction found for {manual_file}\")\n","\n","    # Compute accuracy\n","    if y_true and y_pred:\n","        accuracy = accuracy_score(y_true, y_pred)\n","        report = classification_report(y_true, y_pred, target_names=['Negative (0)', 'Positive (1)'])\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(\"Classification Report:\\n\", report)\n","    else:\n","        print(\"Error: No matching labels found for comparison!\")"],"metadata":{"id":"hxHalW65KioJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/RGB_feature/labelled_images\"\n","manual_folder = \"/content/gdrive/MyDrive/Arecanut_Detection_with_ML/Classification/Trial_Classification/manually_labelled_images\"\n","\n","print(\"Manual folder path:\", manual_folder)\n","\n","# Check if the directory exists\n","if not os.path.exists(manual_folder):\n","  print(f\"Error: Manual folder not found: {manual_folder}\")\n","\n","#Check accuracy against manually labeled images\n","check_accuracy(manual_folder, output_folder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmuexmMOK3N7","executionInfo":{"status":"ok","timestamp":1744279774376,"user_tz":-330,"elapsed":53,"user":{"displayName":"Prajnavi","userId":"02418603915941783168"}},"outputId":"0ef78115-2031-4676-b229-f3ef76989654"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual folder path: /content/gdrive/MyDrive/AI_WRM/Classification/Trial1_Classification/manually_labelled_images\n","Accuracy: 0.7768\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","Negative (0)       0.61      0.92      0.73        37\n","Positive (1)       0.95      0.71      0.81        75\n","\n","    accuracy                           0.78       112\n","   macro avg       0.78      0.81      0.77       112\n","weighted avg       0.83      0.78      0.78       112\n","\n"]}]}]}